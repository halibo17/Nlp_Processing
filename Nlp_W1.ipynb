{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP — Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download Twitter Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk                              \n",
    "from nltk.corpus import twitter_samples  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('twitter_samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "all_negative_tweets = twitter_samples.strings('negative_tweets.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive tweets:  5000\n",
      "Number of negative tweets:  5000\n"
     ]
    }
   ],
   "source": [
    "print('Number of positive tweets: ', len(all_positive_tweets))\n",
    "print('Number of negative tweets: ', len(all_negative_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n"
     ]
    }
   ],
   "source": [
    "print(all_positive_tweets[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "\n",
    "Bütün bir yazıyı oluşturan her bir sözcüğü ayırma işlemidir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize  \n",
    "from nltk.tokenize import TweetTokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#FollowFriday', '@France_Inte', '@PKuchly57', '@Milipol_Paris', 'for', 'being', 'top', 'engaged', 'members', 'in', 'my', 'community', 'this', 'week', ':)']\n"
     ]
    }
   ],
   "source": [
    "print(all_positive_tweets[0].split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#', 'FollowFriday', '@', 'France_Inte', '@', 'PKuchly57', '@', 'Milipol_Paris', 'for', 'being', 'top', 'engaged', 'members', 'in', 'my', 'community', 'this', 'week', ':', ')']\n"
     ]
    }
   ],
   "source": [
    "wrd_tknz = word_tokenize(all_positive_tweets[0])\n",
    "print(wrd_tknz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#FollowFriday', 'for', 'being', 'top', 'engaged', 'members', 'in', 'my', 'community', 'this', 'week', ':)']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = TweetTokenizer(strip_handles=True)\n",
    "\n",
    "# tokenize tweets\n",
    "print(tokenizer.tokenize(all_positive_tweets[0]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization\n",
    "\n",
    "Lemmatization kelimeleri morfolojik olarak inceler. Bir örnek olarak: “Gidiyorlar” gitmek fiilinin üçüncü çoğul şahsının geniş zamanda çekiminden oluşur. Burada kelimenin çekimlenmemiş ilk haline lemma denir, bu örnekte gitmek bir lemmadır. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "# nltk.download('wordnet') \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "  \n",
    "# Create WordNetLemmatizer object \n",
    "wnl = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# ---> #\n",
      "FollowFriday ---> FollowFriday\n",
      "@ ---> @\n",
      "France_Inte ---> France_Inte\n",
      "@ ---> @\n",
      "PKuchly57 ---> PKuchly57\n",
      "@ ---> @\n",
      "Milipol_Paris ---> Milipol_Paris\n",
      "for ---> for\n",
      "being ---> being\n",
      "top ---> top\n",
      "engaged ---> engaged\n",
      "members ---> member\n",
      "in ---> in\n",
      "my ---> my\n",
      "community ---> community\n",
      "this ---> this\n",
      "week ---> week\n",
      ": ---> :\n",
      ") ---> )\n"
     ]
    }
   ],
   "source": [
    "for words in wrd_tknz: \n",
    "    print(words + \" ---> \" + wnl.lemmatize(words)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -U spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# ---> #\n",
      "FollowFriday ---> FollowFriday\n",
      "@France_Inte ---> @france_inte\n",
      "@PKuchly57 ---> @PKuchly57\n",
      "@Milipol_Paris ---> @milipol_paris\n",
      "for ---> for\n",
      "being ---> be\n",
      "top ---> top\n",
      "engaged ---> engaged\n",
      "members ---> member\n",
      "in ---> in\n",
      "my ---> -PRON-\n",
      "community ---> community\n",
      "this ---> this\n",
      "week ---> week\n",
      ":) ---> :)\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "sp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "wrd =sp(all_positive_tweets[0]) \n",
    "\n",
    "for word in wrd:\n",
    "    print(word.text+ \" ---> \"+ word.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FollowFriday ---> FollowFriday\n",
      "France_Inte ---> France_Inte\n",
      "PKuchly57 ---> PKuchly57\n",
      "Milipol_Paris ---> Milipol_Paris\n",
      "for ---> for\n",
      "being ---> being\n",
      "top ---> top\n",
      "engaged ---> engaged\n",
      "members ---> member\n",
      "in ---> in\n",
      "my ---> my\n",
      "community ---> community\n",
      "this ---> this\n",
      "week ---> week\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob, Word \n",
    "  \n",
    "sentence = all_positive_tweets[0]\n",
    "  \n",
    "s = TextBlob(sentence) \n",
    "for w in s.words:\n",
    "    print(w + \" ---> \" + w.lemmatize())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
